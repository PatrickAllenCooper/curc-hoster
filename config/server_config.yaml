# CURC vLLM Server Configuration
# Author: Patrick Cooper
#
# This file defines configurations for different deployment scenarios.
# Reference this file when launching vLLM servers with custom settings.

# Default configuration for small models (7B-13B parameters)
default:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  max_num_seqs: 256
  max_num_batched_tokens: 4096
  host: "0.0.0.0"
  port: 8000

# Configuration for medium models (30B-34B parameters)
medium:
  model: "meta-llama/Llama-3.1-34B-Instruct"
  tensor_parallel_size: 2
  pipeline_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  max_num_seqs: 128
  max_num_batched_tokens: 2048
  host: "0.0.0.0"
  port: 8000

# Configuration for large models (70B parameters, FP16, multi-GPU)
large:
  model: "meta-llama/Llama-3.1-70B-Instruct"
  tensor_parallel_size: 4
  pipeline_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  max_num_seqs: 64
  max_num_batched_tokens: 1024
  host: "0.0.0.0"
  port: 8000

# ============================================================================
# HIGH-END MODELS FOR SINGLE A100 80GB (AWQ 4-BIT QUANTIZED)
# ============================================================================
# These models represent the best open source LLMs that can run on a single
# A100 80GB GPU using AWQ 4-bit quantization (~35-40 GB VRAM)
# ============================================================================

# Qwen 2.5 72B - Top ranked open source model (Feb 2026)
# Memory: ~36 GB with AWQ, 128K context, Apache 2.0 license
# Best overall balance of performance and efficiency
qwen_72b_awq:
  model: "Qwen/Qwen2.5-72B-Instruct-AWQ"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 8192  # Can handle up to 128K but start conservative
  gpu_memory_utilization: 0.9
  max_num_seqs: 32
  max_num_batched_tokens: 4096
  quantization: "awq"
  host: "0.0.0.0"
  port: 8000

# Llama 3.3 70B - Excellent general purpose model
# Memory: ~35 GB with AWQ, strong coding and reasoning
llama_33_70b_awq:
  model: "hugging-quants/Meta-Llama-3.3-70B-Instruct-AWQ-INT4"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 8192
  gpu_memory_utilization: 0.9
  max_num_seqs: 32
  max_num_batched_tokens: 4096
  quantization: "awq"
  host: "0.0.0.0"
  port: 8000

# Llama 3.1 70B - Proven and widely supported
# Memory: ~35 GB with AWQ, extensive community support
llama_31_70b_awq:
  model: "hugging-quants/Meta-Llama-3.1-70B-Instruct-AWQ-INT4"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 8192
  gpu_memory_utilization: 0.9
  max_num_seqs: 32
  max_num_batched_tokens: 4096
  quantization: "awq"
  host: "0.0.0.0"
  port: 8000

# Qwen 2.5 32B - Smaller model with more headroom
# Memory: ~16 GB with AWQ, 128K context, very fast
qwen_32b_awq:
  model: "Qwen/Qwen2.5-32B-Instruct-AWQ"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 16384  # More headroom for longer contexts
  gpu_memory_utilization: 0.85
  max_num_seqs: 64
  max_num_batched_tokens: 8192
  quantization: "awq"
  host: "0.0.0.0"
  port: 8000

# Configuration for very large models (405B parameters)
xlarge:
  model: "meta-llama/Llama-3.1-405B-Instruct"
  tensor_parallel_size: 8
  pipeline_parallel_size: 2
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  max_num_seqs: 32
  max_num_batched_tokens: 512
  host: "0.0.0.0"
  port: 8000

# Configuration for quantized models (reduced memory)
quantized:
  model: "meta-llama/Llama-3.1-70B-Instruct"
  tensor_parallel_size: 2
  pipeline_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.9
  max_num_seqs: 128
  max_num_batched_tokens: 2048
  quantization: "awq"  # Options: awq, gptq, squeezellm
  host: "0.0.0.0"
  port: 8000

# High-throughput configuration (batch processing)
batch:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 2048
  gpu_memory_utilization: 0.95
  max_num_seqs: 512
  max_num_batched_tokens: 8192
  host: "0.0.0.0"
  port: 8000

# Low-latency configuration (interactive use)
interactive:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 4096
  gpu_memory_utilization: 0.8
  max_num_seqs: 64
  max_num_batched_tokens: 2048
  host: "0.0.0.0"
  port: 8000

# Development/testing configuration
dev:
  model: "meta-llama/Llama-3.1-8B-Instruct"
  tensor_parallel_size: 1
  pipeline_parallel_size: 1
  max_model_len: 2048
  gpu_memory_utilization: 0.7
  max_num_seqs: 32
  max_num_batched_tokens: 1024
  host: "0.0.0.0"
  port: 8000

# Slurm job configurations
slurm:
  default:
    nodes: 1
    ntasks: 1
    time: "08:00:00"
    partition: "aa100"
    gres: "gpu:1"
    qos: "normal"
  
  medium:
    nodes: 1
    ntasks: 1
    time: "12:00:00"
    partition: "aa100"
    gres: "gpu:2"
    qos: "normal"
  
  large:
    nodes: 1
    ntasks: 1
    time: "24:00:00"
    partition: "aa100"
    gres: "gpu:4"
    qos: "normal"
  
  xlarge:
    nodes: 2
    ntasks: 2
    time: "24:00:00"
    partition: "aa100"
    gres: "gpu:4"
    qos: "normal"

# Security settings
security:
  # Set to true to require API key authentication
  require_api_key: false
  
  # API key (should be set via environment variable in production)
  api_key_env: "CURC_LLM_API_KEY"
  
  # Allowed origins for CORS (if needed)
  allowed_origins:
    - "http://localhost:*"
    - "http://127.0.0.1:*"

# Logging configuration
logging:
  level: "INFO"  # DEBUG, INFO, WARNING, ERROR, CRITICAL
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  
  # Log files
  server_log: "logs/vllm-server.log"
  access_log: "logs/vllm-access.log"
  error_log: "logs/vllm-error.log"
